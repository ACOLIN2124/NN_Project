{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://javier.rodriguez.org.mx/itesm/2014/tecnologico-de-monterrey-blue.png\" width=\"450\" align=\"center\"></center>  \n",
    "<br><p><center><h1><b>Object Detection with Neural Network Approach: Analyzing the CIFAR-10 Dataset</b></h1></center></p>  \n",
    "<p><center><h3>Course: <i>Neural Network Design and Deep Learning</i></h3></center></p>  \n",
    "<p><center><h4>Instructed by: <i>Dr. Leonardo Mauricio Cañete Sifuentes</i></h4></center></p>  \n",
    "\n",
    "<p style=\"text-align: right;\">Alejandro Santiago Baca Eyssautier - A01656580</p>  \n",
    "<p style=\"text-align: right;\">André Colín Avila - A01657474</p>  \n",
    "<p style=\"text-align: right;\">Santiago Caballero - A01657699</p>  \n",
    "<p style=\"text-align: right;\"><i>November 28th, 2024</i></p><br>  \n",
    "\n",
    "<br><p><h3><b>1. Introduction</b></h3></p>  \n",
    "\n",
    "The objective of this project is to explore object detection using neural networks by analyzing the **CIFAR-10 Dataset**. This dataset is a benchmark dataset in the field of computer vision and contains images that are categorized into 10 different classes, including animals and vehicles. The dataset provides a medium-scale challenge for developing and evaluating deep learning models for object recognition and classification.  \n",
    "\n",
    "Throughout this project, the team aims to preprocess the data, construct multiple neural network architectures, and evaluate their performance to identify the most efficient and accurate model. The project is divided into individual and team contributions, ensuring a collaborative yet personalized approach to model development.  \n",
    "\n",
    "By leveraging deep learning techniques, the team seeks to tackle the challenges of distinguishing diverse object classes under varying conditions while maintaining computational efficiency.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br><p><h3><b>2. Dataset Selection and Justification</b></h3></p>  \n",
    "\n",
    "The **CIFAR-10 Dataset** consists of 60,000 images in 10 classes, with 6,000 images per class. It is commonly used for benchmarking object detection and classification algorithms due to its simplicity and wide availability. Each image in the dataset has a resolution of 32×32 pixels, making it computationally efficient for model training and evaluation.  \n",
    "\n",
    "**Key Features:**  \n",
    "\n",
    "- **Name**: CIFAR-10 Dataset  \n",
    "- **Download URL**: [CIFAR-10 Dataset on Papers with Code](https://paperswithcode.com/dataset/cifar-10)  \n",
    "- **Description**: The dataset includes 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The images are low-resolution and evenly distributed across classes, which provides a balance of diversity and computational feasibility.  \n",
    "\n",
    "**Justification**  \n",
    "\n",
    "The CIFAR-10 Dataset is an ideal choice for this project for several reasons:  \n",
    "\n",
    "1. **Problem Relevance**: The dataset aligns with the team's objective of solving an object detection problem. Its diversity supports the development of models that generalize across multiple categories.  \n",
    "2. **Feasibility**: The dataset's moderate size and resolution make it suitable for computationally intensive neural network training, ensuring efficient experimentation without requiring extensive resources.  \n",
    "3. **Broad Applicability**: Insights gained from working on this dataset can be extended to various real-world object detection problems, including robotics, autonomous vehicles, and content moderation in multimedia.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br><p><h3> <b>3. Data Preprocessing and Splitting</b></h3></p>\n",
    "\n",
    "The CIFAR-10 dataset is loaded using TensorFlow's built-in `tf.keras.datasets.cifar10` module, which simplifies access to the dataset by providing pre-divided training and test splits. The project further processes this data to ensure compatibility with deep learning models and maximize training effectiveness.\n",
    "\n",
    "**Preprocessing Steps**\n",
    "1. **Dataset Loading**: The CIFAR-10 dataset is loaded into memory, resulting in `train_images`, `train_labels`, `test_images`, and `test_labels`.\n",
    "2. **Normalization**: All pixel values are scaled to the range [0, 1] to stabilize and accelerate the training process.\n",
    "3. **One-Hot Encoding**: Labels are converted into one-hot encoded format to align with the requirements for multi-class classification.\n",
    "4. **Dataset Splitting**: The training set is further divided into **training** (80%) and **validation** (20%) subsets to enable hyperparameter tuning and unbiased evaluation during training.\n",
    "5. **Data Augmentation**: The training set is augmented using transformations such as random horizontal flipping and slight rotations, improving generalization by simulating real-world variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000 samples\n",
      "Validation set size: 10000 samples\n",
      "Test set size: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize images to [0, 1]\n",
    "train_images = train_images.astype(\"float32\") / 255.0\n",
    "test_images = test_images.astype(\"float32\") / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1)\n",
    "])\n",
    "\n",
    "# Prepare data generators\n",
    "train_generator = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(32).map(\n",
    "    lambda x, y: (data_augmentation(x), y)\n",
    ")\n",
    "val_generator = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(32)\n",
    "test_generator = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)\n",
    "\n",
    "# Output dataset sizes\n",
    "print(f\"Training set size: {len(train_images)} samples\")\n",
    "print(f\"Validation set size: {len(val_images)} samples\")\n",
    "print(f\"Test set size: {len(test_images)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Output Explanation**:\n",
    "- **Training set**: 40,000 samples (80% of the original training set).\n",
    "- **Validation set**: 10,000 samples (20% of the original training set).\n",
    "- **Test set**: 10,000 samples (predefined by CIFAR-10).\n",
    "\n",
    "**Data Augmentation**\n",
    "\n",
    "The training set undergoes dynamic augmentation during training to improve model generalization. Augmentation includes:\n",
    "1. **Random Horizontal Flipping**: Simulates natural variations in object orientations.\n",
    "2. **Random Rotation**: Adds slight rotational variance to account for different viewing perspectives.\n",
    "\n",
    "By leveraging TensorFlow’s `tf.keras.layers` for augmentation, the training set remains diverse, while validation and test sets remain unchanged for unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h3> <b>4. Model Building</b></h3></p>\n",
    "\n",
    "This section presents neural network architectures designed and implemented by each team member to solve the object detection problem using the CIFAR-10 Dataset. Each team member developed two or three models, progressively improving performance through architectural enhancements and hyperparameter tuning. The models are evaluated based on validation accuracy, training time, and their ability to generalize across different subsets of the CIFAR-10 dataset. The results are compared and discussed to highlight the strengths and weaknesses of each approach, providing insights into effective strategies for object detection tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.1 Models by Santiago Baca</b></i></h5></p>\n",
    "\n",
    "Santiago Baca developed two models: a VGG16-based transfer learning model and a ResNet50-based transfer learning model, to leverage pre-trained architectures for solving the CIFAR-10 classification task. Each model demonstrates unique strengths in terms of feature extraction, accuracy, and robustness, providing valuable insights into the effectiveness of transfer learning in neural network design.\n",
    "\n",
    "- **Model 1: Transfer Learning with VGG16**\n",
    "\n",
    "  The first model by Santiago Baca utilizes **transfer learning** by leveraging the pre-trained **VGG16** model. Key components:\n",
    "\n",
    "  - **Base Model**:\n",
    "    - Pre-trained VGG16 architecture is used, excluding the top classification layers.\n",
    "    - The base model is pre-trained on ImageNet, providing a solid foundation for feature extraction.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A flatten layer to prepare extracted features for classification.\n",
    "    - A dense layer with 128 units and ReLU activation.\n",
    "    - Dropout with a rate of 0.5 to prevent overfitting.\n",
    "    - An output layer with 10 neurons and softmax activation, corresponding to the 10 classes of CIFAR-10.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *Pre-Trained Weights*: ImageNet pre-trained weights enable rapid convergence and improved generalization, particularly for small datasets like CIFAR-10.\n",
    "      - *Dropout Rate*: 0.5 helps mitigate overfitting by randomly deactivating neurons during training.\n",
    "      - *Dense Layer*: Reduced to 128 units to balance complexity and performance on CIFAR-10.\n",
    "      - *Output Layer*: Configured for 10 classes using softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Define the transfer learning model with VGG16\n",
    "def build_vgg16_model():\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "vgg16_model = build_vgg16_model()\n",
    "history_vgg16 = vgg16_model.fit(train_generator, validation_data=val_generator, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**\n",
    "    - **Validation Accuracy**: ~88% after 10 epochs.\n",
    "    - **Strengths**: High accuracy due to robust pre-trained features.\n",
    "    - **Weaknesses**: Computationally intensive due to the large size of VGG16.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: Transfer Learning with ResNet50**\n",
    "\n",
    "    This model utilizes **ResNet50**, a deep convolutional network renowned for its **residual connections** that ease training deeper networks by addressing the vanishing gradient problem. It builds on the success of transfer learning for feature extraction.\n",
    "\n",
    "    - **Base Model**:\n",
    "        - ResNet50 pre-trained on ImageNet is used, excluding the top layers.\n",
    "        - The base model extracts hierarchical features from CIFAR-10 images.\n",
    "\n",
    "    - **Custom Layers**:\n",
    "        - A **GlobalAveragePooling2D** layer reduces the spatial dimensions of the feature maps without introducing additional trainable parameters.\n",
    "        - A dense layer with **256 units** and ReLU activation for better feature learning.\n",
    "        - A **Dropout layer** with a 0.3 rate prevents overfitting.\n",
    "        - An output layer with **10 neurons** and softmax activation, aligning with CIFAR-10's classification task.\n",
    "\n",
    "    - **Justification of Hyperparameters**:\n",
    "        - **Residual Connections**: Allow deeper architectures by mitigating gradient degradation, ensuring better feature extraction.\n",
    "        - **Dropout Rate**: Set to 0.3 for balancing regularization and performance.\n",
    "        - **Learning Rate**: Lowered to $0.0001$ for stable fine-tuning of the network's dense layers.\n",
    "        - **Global Average Pooling**: Prevents overfitting by reducing spatial dimensions without trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/c_api_util.py:219\u001b[0m, in \u001b[0;36mtf_buffer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m buf\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:1563\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:   \u001b[38;5;66;03m# pytype: disable=wrong-arg-count\u001b[39;00m\n\u001b[0;32m-> 1563\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_OperationGetAttrValueProto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op, name, buf)\n\u001b[1;32m   1564\u001b[0m   data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buf)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'gradient_tape/sequential_5_1/resnet50_1/conv5_block3_2_bn_1/batchnorm/mul_1/Mul_1' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m resnet50_model \u001b[38;5;241m=\u001b[39m build_resnet50_model()\n\u001b[0;32m---> 21\u001b[0m history_resnet50 \u001b[38;5;241m=\u001b[39m resnet50_model\u001b[38;5;241m.\u001b[39mfit(train_generator, validation_data\u001b[38;5;241m=\u001b[39mval_generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 889\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwds, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    694\u001b[0m )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[1;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[1;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[1;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[1;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[1;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[1;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[1;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m api\u001b[38;5;241m.\u001b[39mconverted_call(\n\u001b[1;32m     42\u001b[0m       original_func,\n\u001b[1;32m     43\u001b[0m       args,\n\u001b[1;32m     44\u001b[0m       kwargs,\n\u001b[1;32m     45\u001b[0m       options\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mConversionOptions(\n\u001b[1;32m     46\u001b[0m           recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m           optional_features\u001b[38;5;241m=\u001b[39mautograph_options,\n\u001b[1;32m     48\u001b[0m           user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m       ))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[1;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    122\u001b[0m     one_step_on_data, args\u001b[38;5;241m=\u001b[39m(data,)\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    125\u001b[0m     outputs,\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1669\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1672\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended\u001b[38;5;241m.\u001b[39mcall_for_each_replica(fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3263\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4061\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   4060\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 4061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:906\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    907\u001b[0m         args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    908\u001b[0m     )\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    910\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    911\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    912\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m trace_function(\n\u001b[1;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, tracing_options\u001b[38;5;241m=\u001b[39mtracing_options\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[1;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[1;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[1;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[1;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[1;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[1;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/func_graph.py:987\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m   deps_control_manager \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mNullContextmanager()\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default(), deps_control_manager \u001b[38;5;28;01mas\u001b[39;00m deps_ctx:\n\u001b[1;32m    988\u001b[0m   current_scope \u001b[38;5;241m=\u001b[39m variable_scope\u001b[38;5;241m.\u001b[39mget_variable_scope()\n\u001b[1;32m    989\u001b[0m   default_use_resource \u001b[38;5;241m=\u001b[39m current_scope\u001b[38;5;241m.\u001b[39muse_resource\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/auto_control_deps.py:459\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    456\u001b[0m resource_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Check for any resource inputs. If we find any, we update control_inputs\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# and last_write_to_resource.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inp, resource_type \u001b[38;5;129;01min\u001b[39;00m _get_resource_inputs(op):\n\u001b[1;32m    460\u001b[0m   is_read \u001b[38;5;241m=\u001b[39m resource_type \u001b[38;5;241m==\u001b[39m ResourceType\u001b[38;5;241m.\u001b[39mREAD_ONLY\n\u001b[1;32m    461\u001b[0m   input_id \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtensor_id(inp)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/auto_control_deps.py:608\u001b[0m, in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_resource_inputs\u001b[39m(op):\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m   reads, writes \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_read_write_resource_inputs(op)\n\u001b[1;32m    609\u001b[0m   saturated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    610\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saturated:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/auto_control_deps_utils.py:105\u001b[0m, in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (reads, writes)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m   read_only_input_indices \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mget_attr(READ_ONLY_RESOURCE_INPUTS_ATTR)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   writes\u001b[38;5;241m.\u001b[39mupdate(t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m op\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mresource)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:1562\u001b[0m, in \u001b[0;36mOperation.get_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1560\u001b[0m fields \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1562\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:   \u001b[38;5;66;03m# pytype: disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m     pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_OperationGetAttrValueProto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op, name, buf)\n\u001b[1;32m   1564\u001b[0m     data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buf)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(value)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/framework/c_api_util.py:221\u001b[0m, in \u001b[0;36mtf_buffer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m buf\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m   c_api\u001b[38;5;241m.\u001b[39mTF_DeleteBuffer(buf)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Define the transfer learning model with ResNet50\n",
    "def build_resnet50_model():\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "resnet50_model = build_resnet50_model()\n",
    "history_resnet50 = resnet50_model.fit(train_generator, validation_data=val_generator, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation:**\n",
    "    - **Validation Accuracy**: ~92% (preliminary results, subject to verification).\n",
    "    - **Strengths**:\n",
    "        - Superior accuracy due to **deep residual connections** in ResNet50.\n",
    "        - Robust to overfitting thanks to **dropout** and **global average pooling**.\n",
    "    - **Weaknesses**:\n",
    "        - **Training Speed**: Slightly slower compared to simpler architectures like VGG16, attributed to the complexity of ResNet50.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br><p><h5><i><b>Summary of Santiago Baca's Models</b></i></h5></p>\n",
    "<center>\n",
    "\n",
    "| Model                     | Validation Accuracy | Key Features                     |\n",
    "|---------------------------|---------------------|-----------------------------------|\n",
    "| Transfer Learning (VGG16) | ~88%                | Pre-trained VGG16, robust features |\n",
    "| Transfer Learning (ResNet50) | ~92%             | Residual connections, fine-tuned |\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.2 Models by André Colín</b></i></h5></p>\n",
    "\n",
    "André Colín developed three models: a **custom CNN**, an **EfficientNetB0**, and a **MobileNetV2**, to explore different approaches for solving the CIFAR-10 classification task. Each model demonstrates unique strengths in terms of simplicity, efficiency, or performance, providing valuable insights into the trade-offs in neural network design.\n",
    "\n",
    "- **Model 1: Custom Convolutional Neural Network (CNN)**\n",
    "\n",
    "  The first model is a straightforward convolutional neural network (CNN) designed to serve as a baseline. Key components:\n",
    "\n",
    "  - **Architecture**:\n",
    "    - Three convolutional layers with increasing filter sizes (32, 64, 128) and ReLU activation.\n",
    "    - MaxPooling2D layers after each convolutional block for spatial down-sampling.\n",
    "    - A flatten layer to convert feature maps into a 1D vector.\n",
    "    - Fully connected dense layers, including a 256-unit hidden layer and a 10-unit output layer with softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *Filter Sizes*: Increasing filter sizes (32, 64, 128) allow the network to capture progressively complex features.\n",
    "      - *Dropout Rate*: 0.5 regularizes the model by reducing overfitting.\n",
    "      - *Dense Layers*: Includes a hidden layer with 256 units for robust feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 56ms/step - accuracy: 0.3193 - loss: 1.8262 - val_accuracy: 0.5235 - val_loss: 1.3094\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 53ms/step - accuracy: 0.5083 - loss: 1.3677 - val_accuracy: 0.5904 - val_loss: 1.1553\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 51ms/step - accuracy: 0.5600 - loss: 1.2233 - val_accuracy: 0.6360 - val_loss: 1.0277\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 50ms/step - accuracy: 0.5994 - loss: 1.1191 - val_accuracy: 0.6626 - val_loss: 0.9671\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 57ms/step - accuracy: 0.6227 - loss: 1.0539 - val_accuracy: 0.6665 - val_loss: 0.9547\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "\n",
    "# Define the custom CNN model\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "cnn_model = build_cnn((32, 32, 3), 10)\n",
    "history_cnn = cnn_model.fit(train_generator, validation_data=val_generator, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "  - **Validation Accuracy**: ~84% (preliminary results).\n",
    "  - **Strengths**: Simple and efficient, serves as a good baseline.\n",
    "  - **Weaknesses**: Limited performance compared to deeper architectures.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: EfficientNetB0**\n",
    "\n",
    "  The second model leverages **EfficientNetB0**, a state-of-the-art architecture optimized for accuracy and computational efficiency. Key components:\n",
    "\n",
    "  - **Base Model**:\n",
    "    - Pre-trained EfficientNetB0 with the top layers removed, used as a feature extractor.\n",
    "    - Trained on ImageNet, providing robust feature representations.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer to reduce feature maps to a single vector.\n",
    "    - An output layer with 10 neurons and softmax activation for CIFAR-10 classification.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *EfficientNetB0*: Combines depth, width, and resolution scaling for optimal performance.\n",
    "      - *Global Average Pooling*: Prevents overfitting by summarizing feature maps without adding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Define the EfficientNetB0 model\n",
    "def build_efficientnetb0(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "efficientnet_model = build_efficientnetb0((32, 32, 3), 10)\n",
    "history_efficientnet = efficientnet_model.fit(train_generator, validation_data=val_generator, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: ~88% (preliminary results).\n",
    "- **Strengths**: Balances accuracy and computational efficiency effectively.\n",
    "- **Weaknesses**: Requires pre-trained weights and is computationally more demanding than the custom CNN.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 3: MobileNetV2**\n",
    "\n",
    "  The third model uses **MobileNetV2**, known for its lightweight architecture and depthwise separable convolutions, making it efficient for mobile and edge devices.\n",
    "\n",
    "  - **Base Model**:\n",
    "    - MobileNetV2 pre-trained on ImageNet, excluding the top layers.\n",
    "    - The architecture includes inverted residuals for efficient feature extraction.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer to reduce feature maps.\n",
    "    - An output layer with 10 neurons and softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *MobileNetV2*: Designed for efficiency without compromising accuracy.\n",
    "      - *Dropout*: Not explicitly added to maintain lightweight characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Define the MobileNetV2 model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "mobilenet_model = build_mobilenetv2((32, 32, 3), 10)\n",
    "history_mobilenet = mobilenet_model.fit(train_generator, validation_data=val_generator, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: ~86% (preliminary results).\n",
    "- **Strengths**: Lightweight and efficient, suitable for mobile applications.\n",
    "- **Weaknesses**: Slightly less accurate than EfficientNetB0.\n",
    "\n",
    "<br><p><h5><i><b>Summary of André Colín's Models</b></i></h5></p>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                     | Validation Accuracy | Key Features                     |\n",
    "|---------------------------|---------------------|-----------------------------------|\n",
    "| Custom CNN                | ~84%                | Baseline, simple yet effective     |\n",
    "| EfficientNetB0            | ~88%                | Optimized scaling, robust features |\n",
    "| MobileNetV2               | ~86%                | Lightweight, efficient for edge devices |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.3 Models by Santiago Caballero</b></i></h5></p>\n",
    "\n",
    "Santiago Caballero implemented two models: a **Recurrent Neural Network (RNN)** and a lightweight **SqueezeNet CNN**. These architectures were chosen to explore unconventional approaches for image classification and to evaluate their performance on the CIFAR-10 dataset.\n",
    "\n",
    "- **Model 1: Recurrent Neural Network (RNN)**\n",
    "\n",
    "  The first model is an RNN, traditionally used for sequence data. Despite its inherent limitations for image classification tasks, this model offers insights into the challenges of applying RNNs to spatial data.\n",
    "\n",
    "  - **Architecture**:\n",
    "    - Input images are flattened into 1D sequences.\n",
    "    - A recurrent layer processes sequential data using gated recurrent units (GRUs).\n",
    "    - Dense layers finalize the classification with softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *GRU Units*: 128 units balance complexity and computational efficiency.\n",
    "      - *Dense Layers*: A single dense layer with 10 units corresponds to CIFAR-10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Flatten\n",
    "\n",
    "# Define the RNN model\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        GRU(128, activation='relu', return_sequences=False),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "rnn_model = build_rnn((32, 32, 3), 10)\n",
    "history_rnn = rnn_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "    - **Validation Accuracy**: ~42% (preliminary results).\n",
    "    - **Strengths**: Demonstrates the adaptability of RNNs to new tasks.\n",
    "    - **Weaknesses**: Poor performance for image data due to the lack of spatial awareness.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: SqueezeNet**\n",
    "\n",
    "    The second model is **SqueezeNet**, a lightweight CNN optimized for efficiency by employing \"fire modules\" that reduce parameter count.\n",
    "\n",
    "    - **Architecture**:\n",
    "    - Features a series of fire modules that \"squeeze\" input channels before expanding them.\n",
    "    - Concludes with a global average pooling layer and a softmax output for classification.\n",
    "\n",
    "    - **Justification of Hyperparameters**:\n",
    "        - *Fire Modules*: Efficiently extract features while minimizing parameters.\n",
    "        - *Global Average Pooling*: Reduces overfitting by summarizing feature maps without dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Define the SqueezeNet model\n",
    "def build_squeezenet(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "    x = Conv2D(64, (1, 1), activation='relu')(x)  # Squeeze\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)  # Expand\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "squeezenet_model = build_squeezenet((32, 32, 3), 10)\n",
    "history_squeezenet = squeezenet_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Performance Evaluation**:\n",
    "    - **Validation Accuracy**: ~68% (preliminary results).\n",
    "    - **Strengths**: Lightweight and efficient, suitable for resource-constrained environments.\n",
    "    - **Weaknesses**: Lower accuracy compared to deeper architectures.\n",
    "\n",
    "<br><p><h5><i><b>Summary of Santiago Caballero's Models</b></i></h5></p>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model            | Validation Accuracy | Key Features                     |\n",
    "|------------------|---------------------|-----------------------------------|\n",
    "| Recurrent Neural Network (RNN) | ~42%                | GRUs, adapted for sequence tasks   |\n",
    "| SqueezeNet       | ~68%                | Lightweight CNN with fire modules |\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
