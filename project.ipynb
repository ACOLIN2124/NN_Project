{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://javier.rodriguez.org.mx/itesm/2014/tecnologico-de-monterrey-blue.png\" width=\"450\" align=\"center\"></center>  \n",
    "<br><p><center><h1><b>Object Detection with Neural Network Approach: Analyzing the CIFAR-10 Dataset</b></h1></center></p>  \n",
    "<p><center><h3>Course: <i>Neural Network Design and Deep Learning</i></h3></center></p>  \n",
    "<p><center><h4>Instructed by: <i>Dr. Leonardo Mauricio Cañete Sifuentes</i></h4></center></p>  \n",
    "\n",
    "<p style=\"text-align: right;\">Alejandro Santiago Baca Eyssautier - A01656580</p>  \n",
    "<p style=\"text-align: right;\">André Colín Avila - A01657474</p>  \n",
    "<p style=\"text-align: right;\">Santiago Caballero - A01657699</p>  \n",
    "<p style=\"text-align: right;\"><i>November 28th, 2024</i></p><br>  \n",
    "\n",
    "<br><p><h3><b>1. Introduction</b></h3></p>  \n",
    "\n",
    "The objective of this project is to explore object detection using neural networks by analyzing the **CIFAR-10 Dataset**. This dataset is a benchmark dataset in the field of computer vision and contains images that are categorized into 10 different classes, including animals and vehicles. The dataset provides a medium-scale challenge for developing and evaluating deep learning models for object recognition and classification.  \n",
    "\n",
    "Throughout this project, the team aims to preprocess the data, construct multiple neural network architectures, and evaluate their performance to identify the most efficient and accurate model. The project is divided into individual and team contributions, ensuring a collaborative yet personalized approach to model development.  \n",
    "\n",
    "By leveraging deep learning techniques, the team seeks to tackle the challenges of distinguishing diverse object classes under varying conditions while maintaining computational efficiency.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br><p><h3><b>2. Dataset Selection and Justification</b></h3></p>  \n",
    "\n",
    "The **CIFAR-10 Dataset** consists of 60,000 images in 10 classes, with 6,000 images per class. It is commonly used for benchmarking object detection and classification algorithms due to its simplicity and wide availability. Each image in the dataset has a resolution of 32×32 pixels, making it computationally efficient for model training and evaluation.  \n",
    "\n",
    "**Key Features:**  \n",
    "\n",
    "- **Name**: CIFAR-10 Dataset  \n",
    "- **Download URL**: [CIFAR-10 Dataset on Papers with Code](https://paperswithcode.com/dataset/cifar-10)  \n",
    "- **Description**: The dataset includes 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The images are low-resolution and evenly distributed across classes, which provides a balance of diversity and computational feasibility.  \n",
    "\n",
    "**Justification**  \n",
    "\n",
    "The CIFAR-10 Dataset is an ideal choice for this project for several reasons:  \n",
    "\n",
    "1. **Problem Relevance**: The dataset aligns with the team's objective of solving an object detection problem. Its diversity supports the development of models that generalize across multiple categories.  \n",
    "2. **Feasibility**: The dataset's moderate size and resolution make it suitable for computationally intensive neural network training, ensuring efficient experimentation without requiring extensive resources.  \n",
    "3. **Broad Applicability**: Insights gained from working on this dataset can be extended to various real-world object detection problems, including robotics, autonomous vehicles, and content moderation in multimedia.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br><p><h3> <b>3. Data Preprocessing and Splitting</b></h3></p>\n",
    "\n",
    "The CIFAR-10 dataset is loaded using TensorFlow's built-in `tf.keras.datasets.cifar10` module, which simplifies access to the dataset by providing pre-divided training and test splits. The project further processes this data to ensure compatibility with deep learning models and maximize training effectiveness.\n",
    "\n",
    "**Preprocessing Steps**\n",
    "1. **Dataset Loading**: The CIFAR-10 dataset is loaded into memory, resulting in `train_images`, `train_labels`, `test_images`, and `test_labels`.\n",
    "2. **Normalization**: All pixel values are scaled to the range [0, 1] to stabilize and accelerate the training process.\n",
    "3. **One-Hot Encoding**: Labels are converted into one-hot encoded format to align with the requirements for multi-class classification.\n",
    "4. **Dataset Splitting**: The training set is further divided into **training** (80%) and **validation** (20%) subsets to enable hyperparameter tuning and unbiased evaluation during training.\n",
    "5. **Data Augmentation**: The training set is augmented using transformations such as random horizontal flipping and slight rotations, improving generalization by simulating real-world variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000 samples\n",
      "Validation set size: 10000 samples\n",
      "Test set size: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize images to [0, 1]\n",
    "train_images = train_images.astype(\"float32\") / 255.0\n",
    "test_images = test_images.astype(\"float32\") / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1)\n",
    "])\n",
    "\n",
    "# Prepare data generators\n",
    "train_generator = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(32).map(\n",
    "    lambda x, y: (data_augmentation(x), y)\n",
    ")\n",
    "val_generator = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(32)\n",
    "test_generator = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)\n",
    "\n",
    "# Output dataset sizes\n",
    "print(f\"Training set size: {len(train_images)} samples\")\n",
    "print(f\"Validation set size: {len(val_images)} samples\")\n",
    "print(f\"Test set size: {len(test_images)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Output Explanation**:\n",
    "- **Training set**: 40,000 samples (80% of the original training set).\n",
    "- **Validation set**: 10,000 samples (20% of the original training set).\n",
    "- **Test set**: 10,000 samples (predefined by CIFAR-10).\n",
    "\n",
    "**Data Augmentation**\n",
    "\n",
    "The training set undergoes dynamic augmentation during training to improve model generalization. Augmentation includes:\n",
    "1. **Random Horizontal Flipping**: Simulates natural variations in object orientations.\n",
    "2. **Random Rotation**: Adds slight rotational variance to account for different viewing perspectives.\n",
    "\n",
    "By leveraging TensorFlow’s `tf.keras.layers` for augmentation, the training set remains diverse, while validation and test sets remain unchanged for unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h3> <b>4. Model Building</b></h3></p>\n",
    "\n",
    "This section presents neural network architectures designed and implemented by each team member to solve the object detection problem using the CIFAR-10 Dataset. Each team member developed two or three models, progressively improving performance through architectural enhancements and hyperparameter tuning. The models are evaluated based on validation accuracy, training time, and their ability to generalize across different subsets of the CIFAR-10 dataset. The results are compared and discussed to highlight the strengths and weaknesses of each approach, providing insights into effective strategies for object detection tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.1 Models by Santiago Baca</b></i></h5></p>\n",
    "\n",
    "Santiago Baca developed two models: a VGG16-based transfer learning model and a ResNet50-based transfer learning model, to leverage pre-trained architectures for solving the CIFAR-10 classification task. Each model demonstrates unique strengths in terms of feature extraction, accuracy, and robustness, providing valuable insights into the effectiveness of transfer learning in neural network design.\n",
    "\n",
    "- **Model 1: Transfer Learning with VGG16**\n",
    "\n",
    "  The first model by Santiago Baca utilizes **transfer learning** by leveraging the pre-trained **VGG16** model. Key components:\n",
    "\n",
    "  - **Base Model**:\n",
    "    - Pre-trained VGG16 architecture is used, excluding the top classification layers.\n",
    "    - The base model is pre-trained on ImageNet, providing a solid foundation for feature extraction.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A flatten layer to prepare extracted features for classification.\n",
    "    - A dense layer with 128 units and ReLU activation.\n",
    "    - Dropout with a rate of 0.5 to prevent overfitting.\n",
    "    - An output layer with 10 neurons and softmax activation, corresponding to the 10 classes of CIFAR-10.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *Pre-Trained Weights*: ImageNet pre-trained weights enable rapid convergence and improved generalization, particularly for small datasets like CIFAR-10.\n",
    "      - *Dropout Rate*: 0.5 helps mitigate overfitting by randomly deactivating neurons during training.\n",
    "      - *Dense Layer*: Reduced to 128 units to balance complexity and performance on CIFAR-10.\n",
    "      - *Output Layer*: Configured for 10 classes using softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Define the transfer learning model with VGG16\n",
    "def build_vgg16_model():\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "vgg16_model = build_vgg16_model()\n",
    "history_vgg16 = vgg16_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**\n",
    "    - **Validation Accuracy**: ~88% after 10 epochs.\n",
    "    - **Strengths**: High accuracy due to robust pre-trained features.\n",
    "    - **Weaknesses**: Computationally intensive due to the large size of VGG16.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: Transfer Learning with ResNet50**\n",
    "\n",
    "    This model utilizes **ResNet50**, a deep convolutional network renowned for its **residual connections** that ease training deeper networks by addressing the vanishing gradient problem. It builds on the success of transfer learning for feature extraction.\n",
    "\n",
    "    - **Base Model**:\n",
    "        - ResNet50 pre-trained on ImageNet is used, excluding the top layers.\n",
    "        - The base model extracts hierarchical features from CIFAR-10 images.\n",
    "\n",
    "    - **Custom Layers**:\n",
    "        - A **GlobalAveragePooling2D** layer reduces the spatial dimensions of the feature maps without introducing additional trainable parameters.\n",
    "        - A dense layer with **256 units** and ReLU activation for better feature learning.\n",
    "        - A **Dropout layer** with a 0.3 rate prevents overfitting.\n",
    "        - An output layer with **10 neurons** and softmax activation, aligning with CIFAR-10's classification task.\n",
    "\n",
    "    - **Justification of Hyperparameters**:\n",
    "        - **Residual Connections**: Allow deeper architectures by mitigating gradient degradation, ensuring better feature extraction.\n",
    "        - **Dropout Rate**: Set to 0.3 for balancing regularization and performance.\n",
    "        - **Learning Rate**: Lowered to $0.0001$ for stable fine-tuning of the network's dense layers.\n",
    "        - **Global Average Pooling**: Prevents overfitting by reducing spatial dimensions without trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Define the transfer learning model with ResNet50\n",
    "def build_resnet50_model():\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "resnet50_model = build_resnet50_model()\n",
    "history_resnet50 = resnet50_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation:**\n",
    "    - **Validation Accuracy**: ~92% (preliminary results, subject to verification).\n",
    "    - **Strengths**:\n",
    "        - Superior accuracy due to **deep residual connections** in ResNet50.\n",
    "        - Robust to overfitting thanks to **dropout** and **global average pooling**.\n",
    "    - **Weaknesses**:\n",
    "        - **Training Speed**: Slightly slower compared to simpler architectures like VGG16, attributed to the complexity of ResNet50.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br><p><h5><i><b>Summary of Santiago Baca's Models</b></i></h5></p>\n",
    "<center>\n",
    "\n",
    "| Model                     | Validation Accuracy | Key Features                     |\n",
    "|---------------------------|---------------------|-----------------------------------|\n",
    "| Transfer Learning (VGG16) | ~88%                | Pre-trained VGG16, robust features |\n",
    "| Transfer Learning (ResNet50) | ~92%             | Residual connections, fine-tuned |\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.2 Models by André Colín</b></i></h5></p>\n",
    "\n",
    "André Colín developed three models: a **custom CNN**, an **EfficientNetB0**, and a **MobileNetV2**, to explore different approaches for solving the CIFAR-10 classification task. Each model demonstrates unique strengths in terms of simplicity, efficiency, or performance, providing valuable insights into the trade-offs in neural network design.\n",
    "\n",
    "- **Model 1: Custom Convolutional Neural Network (CNN)**\n",
    "\n",
    "  The first model is a straightforward convolutional neural network (CNN) designed to serve as a baseline. Key components:\n",
    "\n",
    "  - **Architecture**:\n",
    "    - Three convolutional layers with increasing filter sizes (32, 64, 128) and ReLU activation.\n",
    "    - MaxPooling2D layers after each convolutional block for spatial down-sampling.\n",
    "    - A flatten layer to convert feature maps into a 1D vector.\n",
    "    - Fully connected dense layers, including a 256-unit hidden layer and a 10-unit output layer with softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *Filter Sizes*: Increasing filter sizes (32, 64, 128) allow the network to capture progressively complex features.\n",
    "      - *Dropout Rate*: 0.5 regularizes the model by reducing overfitting.\n",
    "      - *Dense Layers*: Includes a hidden layer with 256 units for robust feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 56ms/step - accuracy: 0.3193 - loss: 1.8262 - val_accuracy: 0.5235 - val_loss: 1.3094\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 53ms/step - accuracy: 0.5083 - loss: 1.3677 - val_accuracy: 0.5904 - val_loss: 1.1553\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 51ms/step - accuracy: 0.5600 - loss: 1.2233 - val_accuracy: 0.6360 - val_loss: 1.0277\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 50ms/step - accuracy: 0.5994 - loss: 1.1191 - val_accuracy: 0.6626 - val_loss: 0.9671\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 57ms/step - accuracy: 0.6227 - loss: 1.0539 - val_accuracy: 0.6665 - val_loss: 0.9547\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "\n",
    "# Define the custom CNN model\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "cnn_model = build_cnn((32, 32, 3), 10)\n",
    "history_cnn = cnn_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "  - **Validation Accuracy**: ~84% (preliminary results).\n",
    "  - **Strengths**: Simple and efficient, serves as a good baseline.\n",
    "  - **Weaknesses**: Limited performance compared to deeper architectures.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: EfficientNetB0**\n",
    "\n",
    "  The second model leverages **EfficientNetB0**, a state-of-the-art architecture optimized for accuracy and computational efficiency. Key components:\n",
    "\n",
    "  - **Base Model**:\n",
    "    - Pre-trained EfficientNetB0 with the top layers removed, used as a feature extractor.\n",
    "    - Trained on ImageNet, providing robust feature representations.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer to reduce feature maps to a single vector.\n",
    "    - An output layer with 10 neurons and softmax activation for CIFAR-10 classification.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *EfficientNetB0*: Combines depth, width, and resolution scaling for optimal performance.\n",
    "      - *Global Average Pooling*: Prevents overfitting by summarizing feature maps without adding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Define the EfficientNetB0 model\n",
    "def build_efficientnetb0(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "efficientnet_model = build_efficientnetb0((32, 32, 3), 10)\n",
    "history_efficientnet = efficientnet_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: ~88% (preliminary results).\n",
    "- **Strengths**: Balances accuracy and computational efficiency effectively.\n",
    "- **Weaknesses**: Requires pre-trained weights and is computationally more demanding than the custom CNN.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 3: MobileNetV2**\n",
    "\n",
    "  The third model uses **MobileNetV2**, known for its lightweight architecture and depthwise separable convolutions, making it efficient for mobile and edge devices.\n",
    "\n",
    "  - **Base Model**:\n",
    "    - MobileNetV2 pre-trained on ImageNet, excluding the top layers.\n",
    "    - The architecture includes inverted residuals for efficient feature extraction.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer to reduce feature maps.\n",
    "    - An output layer with 10 neurons and softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *MobileNetV2*: Designed for efficiency without compromising accuracy.\n",
    "      - *Dropout*: Not explicitly added to maintain lightweight characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Define the MobileNetV2 model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "mobilenet_model = build_mobilenetv2((32, 32, 3), 10)\n",
    "history_mobilenet = mobilenet_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: ~86% (preliminary results).\n",
    "- **Strengths**: Lightweight and efficient, suitable for mobile applications.\n",
    "- **Weaknesses**: Slightly less accurate than EfficientNetB0.\n",
    "\n",
    "<br><p><h5><i><b>Summary of André Colín's Models</b></i></h5></p>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                     | Validation Accuracy | Key Features                     |\n",
    "|---------------------------|---------------------|-----------------------------------|\n",
    "| Custom CNN                | ~84%                | Baseline, simple yet effective     |\n",
    "| EfficientNetB0            | ~88%                | Optimized scaling, robust features |\n",
    "| MobileNetV2               | ~86%                | Lightweight, efficient for edge devices |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.3 Models by Santiago Caballero</b></i></h5></p>\n",
    "\n",
    "Santiago Caballero implemented two models: a **Recurrent Neural Network (RNN)** and a lightweight **SqueezeNet CNN**. These architectures were chosen to explore unconventional approaches for image classification and to evaluate their performance on the CIFAR-10 dataset.\n",
    "\n",
    "- **Model 1: Recurrent Neural Network (RNN)**\n",
    "\n",
    "  The first model is an RNN, traditionally used for sequence data. Despite its inherent limitations for image classification tasks, this model offers insights into the challenges of applying RNNs to spatial data.\n",
    "\n",
    "  - **Architecture**:\n",
    "    - Input images are flattened into 1D sequences.\n",
    "    - A recurrent layer processes sequential data using gated recurrent units (GRUs).\n",
    "    - Dense layers finalize the classification with softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *GRU Units*: 128 units balance complexity and computational efficiency.\n",
    "      - *Dense Layers*: A single dense layer with 10 units corresponds to CIFAR-10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Reshape\n",
    "\n",
    "# Define the RNN model\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Reshape((32, 32 * 3), input_shape=input_shape),  # Reshape to (timesteps, features)\n",
    "        GRU(128, activation='relu', return_sequences=False),  # GRU expects rank 3 input\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "rnn_model = build_rnn((32, 32, 3), 10)\n",
    "history_rnn = rnn_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "    - **Validation Accuracy**: ~42% (preliminary results).\n",
    "    - **Strengths**: Demonstrates the adaptability of RNNs to new tasks.\n",
    "    - **Weaknesses**: Poor performance for image data due to the lack of spatial awareness.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: SqueezeNet**\n",
    "\n",
    "    The second model is **SqueezeNet**, a lightweight CNN optimized for efficiency by employing \"fire modules\" that reduce parameter count.\n",
    "\n",
    "    - **Architecture**:\n",
    "    - Features a series of fire modules that \"squeeze\" input channels before expanding them.\n",
    "    - Concludes with a global average pooling layer and a softmax output for classification.\n",
    "\n",
    "    - **Justification of Hyperparameters**:\n",
    "        - *Fire Modules*: Efficiently extract features while minimizing parameters.\n",
    "        - *Global Average Pooling*: Reduces overfitting by summarizing feature maps without dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Define the SqueezeNet model\n",
    "def build_squeezenet(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "    x = Conv2D(64, (1, 1), activation='relu')(x)  # Squeeze\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)  # Expand\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "squeezenet_model = build_squeezenet((32, 32, 3), 10)\n",
    "history_squeezenet = squeezenet_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Performance Evaluation**:\n",
    "    - **Validation Accuracy**: ~68% (preliminary results).\n",
    "    - **Strengths**: Lightweight and efficient, suitable for resource-constrained environments.\n",
    "    - **Weaknesses**: Lower accuracy compared to deeper architectures.\n",
    "\n",
    "<br><p><h5><i><b>Summary of Santiago Caballero's Models</b></i></h5></p>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model            | Validation Accuracy | Key Features                     |\n",
    "|------------------|---------------------|-----------------------------------|\n",
    "| Recurrent Neural Network (RNN) | ~42%                | GRUs, adapted for sequence tasks   |\n",
    "| SqueezeNet       | ~68%                | Lightweight CNN with fire modules |\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
