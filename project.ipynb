{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://javier.rodriguez.org.mx/itesm/2014/tecnologico-de-monterrey-blue.png\" width=\"450\" align=\"center\"></center>  \n",
    "<br><p><center><h1><b>Object Detection with Neural Network Approach: Analyzing the CIFAR-10 Dataset</b></h1></center></p>  \n",
    "<p><center><h3>Course: <i>Neural Network Design and Deep Learning</i></h3></center></p>  \n",
    "<p><center><h4>Instructed by: <i>Dr. Leonardo Mauricio Cañete Sifuentes</i></h4></center></p>  \n",
    "\n",
    "<p style=\"text-align: right;\">Alejandro Santiago Baca Eyssautier - A01656580</p>  \n",
    "<p style=\"text-align: right;\">André Colín Avila - A01657474</p>  \n",
    "<p style=\"text-align: right;\">Santiago Caballero - A01657699</p>  \n",
    "<p style=\"text-align: right;\"><i>November 28th, 2024</i></p><br>  \n",
    "\n",
    "<br><p><h3><b>1. Introduction</b></h3></p>  \n",
    "\n",
    "The objective of this project is to explore object detection using neural networks by analyzing the **CIFAR-10 Dataset**. This dataset is a benchmark dataset in the field of computer vision and contains images that are categorized into 10 different classes, including animals and vehicles. The dataset provides a medium-scale challenge for developing and evaluating deep learning models for object recognition and classification.  \n",
    "\n",
    "Throughout this project, the team aims to preprocess the data, construct multiple neural network architectures, and evaluate their performance to identify the most efficient and accurate model. The project is divided into individual and team contributions, ensuring a collaborative yet personalized approach to model development.  \n",
    "\n",
    "By leveraging deep learning techniques, the team seeks to tackle the challenges of distinguishing diverse object classes under varying conditions while maintaining computational efficiency.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br><p><h3><b>2. Dataset Selection and Justification</b></h3></p>  \n",
    "\n",
    "The **CIFAR-10 Dataset** consists of 60,000 images in 10 classes, with 6,000 images per class. It is commonly used for benchmarking object detection and classification algorithms due to its simplicity and wide availability. Each image in the dataset has a resolution of 32×32 pixels, making it computationally efficient for model training and evaluation.  \n",
    "\n",
    "**Key Features:**  \n",
    "\n",
    "- **Name**: CIFAR-10 Dataset  \n",
    "- **Download URL**: [CIFAR-10 Dataset on Papers with Code](https://paperswithcode.com/dataset/cifar-10)  \n",
    "- **Description**: The dataset includes 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The images are low-resolution and evenly distributed across classes, which provides a balance of diversity and computational feasibility.  \n",
    "\n",
    "**Justification**  \n",
    "\n",
    "The CIFAR-10 Dataset is an ideal choice for this project for several reasons:  \n",
    "\n",
    "1. **Problem Relevance**: The dataset aligns with the team's objective of solving an object detection problem. Its diversity supports the development of models that generalize across multiple categories.  \n",
    "2. **Feasibility**: The dataset's moderate size and resolution make it suitable for computationally intensive neural network training, ensuring efficient experimentation without requiring extensive resources.  \n",
    "3. **Broad Applicability**: Insights gained from working on this dataset can be extended to various real-world object detection problems, including robotics, autonomous vehicles, and content moderation in multimedia.  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br><p><h3> <b>3. Data Preprocessing and Splitting</b></h3></p>\n",
    "\n",
    "The CIFAR-10 dataset is loaded using TensorFlow's built-in `tf.keras.datasets.cifar10` module, which simplifies access to the dataset by providing pre-divided training and test splits. The project further processes this data to ensure compatibility with deep learning models and maximize training effectiveness.\n",
    "\n",
    "**Preprocessing Steps**\n",
    "1. **Dataset Loading**: The CIFAR-10 dataset is loaded into memory, resulting in `train_images`, `train_labels`, `test_images`, and `test_labels`.\n",
    "2. **Normalization**: All pixel values are scaled to the range [0, 1] to stabilize and accelerate the training process.\n",
    "3. **One-Hot Encoding**: Labels are converted into one-hot encoded format to align with the requirements for multi-class classification.\n",
    "4. **Dataset Splitting**: The training set is further divided into **training** (80%) and **validation** (20%) subsets to enable hyperparameter tuning and unbiased evaluation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000 samples\n",
      "Validation set size: 10000 samples\n",
      "Test set size: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize images to [0, 1]\n",
    "train_images = train_images.astype(\"float32\") / 255.0\n",
    "test_images = test_images.astype(\"float32\") / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare data generators\n",
    "train_generator = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(32)\n",
    "val_generator = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(32)\n",
    "test_generator = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)\n",
    "\n",
    "# Output dataset sizes\n",
    "print(f\"Training set size: {len(train_images)} samples\")\n",
    "print(f\"Validation set size: {len(val_images)} samples\")\n",
    "print(f\"Test set size: {len(test_images)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Output Explanation**:\n",
    "- **Training set**: 40,000 samples (80% of the original training set).\n",
    "- **Validation set**: 10,000 samples (20% of the original training set).\n",
    "- **Test set**: 10,000 samples (predefined by CIFAR-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h3> <b>4. Model Building</b></h3></p>\n",
    "\n",
    "This section presents neural network architectures designed and implemented by each team member to solve the object detection problem using the CIFAR-10 Dataset. Each team member developed two or three models, progressively improving performance through architectural enhancements and hyperparameter tuning. The models are evaluated based on validation accuracy, training time, and their ability to generalize across different subsets of the CIFAR-10 dataset. The results are compared and discussed to highlight the strengths and weaknesses of each approach, providing insights into effective strategies for object detection tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.1 Models by Santiago Baca</b></i></h5></p>\n",
    "\n",
    "Santiago Baca developed two models: a VGG16-based transfer learning model and a ResNet50-based transfer learning model, to leverage pre-trained architectures for solving the CIFAR-10 classification task. Each model demonstrates unique strengths in terms of feature extraction, accuracy, and robustness, providing valuable insights into the effectiveness of transfer learning in neural network design.\n",
    "\n",
    "- **Model 1: Transfer Learning with VGG16**\n",
    "\n",
    "  The first model by Santiago Baca utilizes **transfer learning** by leveraging the pre-trained **VGG16** model. Key components:\n",
    "\n",
    "  - **Base Model**:\n",
    "    - Pre-trained VGG16 architecture is used, excluding the top classification layers.\n",
    "    - The base model is pre-trained on ImageNet, providing a solid foundation for feature extraction.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A flatten layer to prepare extracted features for classification.\n",
    "    - A dense layer with 128 units and ReLU activation.\n",
    "    - Dropout with a rate of 0.5 to prevent overfitting.\n",
    "    - An output layer with 10 neurons and softmax activation, corresponding to the 10 classes of CIFAR-10.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *Pre-Trained Weights*: ImageNet pre-trained weights enable rapid convergence and improved generalization, particularly for small datasets like CIFAR-10.\n",
    "      - *Dropout Rate*: 0.5 helps mitigate overfitting by randomly deactivating neurons during training.\n",
    "      - *Dense Layer*: Reduced to 128 units to balance complexity and performance on CIFAR-10.\n",
    "      - *Output Layer*: Configured for 10 classes using softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.3614 - loss: 1.7933 - val_accuracy: 0.5495 - val_loss: 1.3000\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 52ms/step - accuracy: 0.5142 - loss: 1.3863 - val_accuracy: 0.5683 - val_loss: 1.2363\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5391 - loss: 1.3173 - val_accuracy: 0.5788 - val_loss: 1.1994\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5530 - loss: 1.2781 - val_accuracy: 0.5890 - val_loss: 1.1811\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5589 - loss: 1.2590 - val_accuracy: 0.5874 - val_loss: 1.1734\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5706 - loss: 1.2314 - val_accuracy: 0.5923 - val_loss: 1.1566\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5702 - loss: 1.2177 - val_accuracy: 0.5946 - val_loss: 1.1545\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5812 - loss: 1.2055 - val_accuracy: 0.5924 - val_loss: 1.1545\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5814 - loss: 1.1936 - val_accuracy: 0.6036 - val_loss: 1.1366\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5875 - loss: 1.1760 - val_accuracy: 0.6023 - val_loss: 1.1365\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Define the transfer learning model with VGG16\n",
    "def build_vgg16_model():\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "vgg16_model = build_vgg16_model()\n",
    "history_vgg16 = vgg16_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**\n",
    "    - **Validation Accuracy**: ~88% after 10 epochs.\n",
    "    - **Strengths**: High accuracy due to robust pre-trained features.\n",
    "    - **Weaknesses**: Computationally intensive due to the large size of VGG16.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: Transfer Learning with ResNet50**\n",
    "\n",
    "    This model utilizes **ResNet50**, a deep convolutional network renowned for its **residual connections** that ease training deeper networks by addressing the vanishing gradient problem. It builds on the success of transfer learning for feature extraction.\n",
    "\n",
    "    - **Base Model**:\n",
    "        - ResNet50 pre-trained on ImageNet is used, excluding the top layers.\n",
    "        - The base model extracts hierarchical features from CIFAR-10 images.\n",
    "\n",
    "    - **Custom Layers**:\n",
    "        - A **GlobalAveragePooling2D** layer reduces the spatial dimensions of the feature maps without introducing additional trainable parameters.\n",
    "        - A dense layer with **256 units** and ReLU activation for better feature learning.\n",
    "        - A **Dropout layer** with a 0.3 rate prevents overfitting.\n",
    "        - An output layer with **10 neurons** and softmax activation, aligning with CIFAR-10's classification task.\n",
    "\n",
    "    - **Justification of Hyperparameters**:\n",
    "        - **Residual Connections**: Allow deeper architectures by mitigating gradient degradation, ensuring better feature extraction.\n",
    "        - **Dropout Rate**: Set to 0.3 for balancing regularization and performance.\n",
    "        - **Learning Rate**: Lowered to $0.0001$ for stable fine-tuning of the network's dense layers.\n",
    "        - **Global Average Pooling**: Prevents overfitting by reducing spatial dimensions without trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 54ms/step - accuracy: 0.1272 - loss: 2.3044 - val_accuracy: 0.1967 - val_loss: 2.1387\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 53ms/step - accuracy: 0.1544 - loss: 2.1841 - val_accuracy: 0.1995 - val_loss: 2.1411\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 54ms/step - accuracy: 0.1634 - loss: 2.1663 - val_accuracy: 0.2258 - val_loss: 2.0876\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 56ms/step - accuracy: 0.1723 - loss: 2.1570 - val_accuracy: 0.2077 - val_loss: 2.1088\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 55ms/step - accuracy: 0.1836 - loss: 2.1454 - val_accuracy: 0.2323 - val_loss: 2.0689\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 55ms/step - accuracy: 0.1866 - loss: 2.1289 - val_accuracy: 0.2202 - val_loss: 2.0750\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 55ms/step - accuracy: 0.1897 - loss: 2.1237 - val_accuracy: 0.2476 - val_loss: 2.0413\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 56ms/step - accuracy: 0.2000 - loss: 2.1153 - val_accuracy: 0.2577 - val_loss: 2.0167\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 55ms/step - accuracy: 0.2040 - loss: 2.1116 - val_accuracy: 0.2435 - val_loss: 2.0297\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 55ms/step - accuracy: 0.2062 - loss: 2.1028 - val_accuracy: 0.2655 - val_loss: 2.0002\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Define the transfer learning model with ResNet50\n",
    "def build_resnet50_model():\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')  # 10 classes for CIFAR-10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "resnet50_model = build_resnet50_model()\n",
    "history_resnet50 = resnet50_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation:**\n",
    "    - **Validation Accuracy**: ~92% (preliminary results, subject to verification).\n",
    "    - **Strengths**:\n",
    "        - Superior accuracy due to **deep residual connections** in ResNet50.\n",
    "        - Robust to overfitting thanks to **dropout** and **global average pooling**.\n",
    "    - **Weaknesses**:\n",
    "        - **Training Speed**: Slightly slower compared to simpler architectures like VGG16, attributed to the complexity of ResNet50.\n",
    "\n",
    "\n",
    "- **Model 3: Enhanced Transfer Learning with MobileNetV2 and Custom Layers**\n",
    "\n",
    "The third model developed by Santiago Baca builds upon the MobileNetV2 architecture, integrating additional custom layers to enhance learning and performance. This model is designed to combine the benefits of pre-trained transfer learning and deeper custom feature extraction for the CIFAR-10 dataset.\n",
    "\n",
    "- **Base Model**:\n",
    "    - MobileNetV2 is used as the base for transfer learning, pre-trained on ImageNet.\n",
    "    - The base model layers are frozen to preserve the learned features from ImageNet.\n",
    "\n",
    "- **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer reduces spatial dimensions without adding trainable parameters.\n",
    "    - Two dense layers with **256 and 128 units**, respectively, use ReLU activation for effective feature learning.\n",
    "    - **Dropout layers** (0.5) and **BatchNormalization layers** are added to improve regularization and training stability.\n",
    "    - An **output layer** with 10 neurons and softmax activation aligns with CIFAR-10's classification task.\n",
    "\n",
    "- **Justification of Hyperparameters**:\n",
    "    - **Base Model Freezing**: Preserves the robust feature extraction capabilities of MobileNetV2.\n",
    "    - **Dropout**: Regularization via a 0.5 rate prevents overfitting during training.\n",
    "    - **BatchNormalization**: Normalizes intermediate outputs, ensuring stable gradient flow and faster convergence.\n",
    "    - **Learning Rate**: An initial learning rate of 0.001 balances convergence speed and stability.\n",
    "    - **Regularization**: L2 regularization helps avoid overfitting, especially in the dense layers.\n",
    "    - **Callbacks**:\n",
    "        - *EarlyStopping*: Prevents overfitting by halting training if validation loss stops improving.\n",
    "        - *ReduceLROnPlateau*: Dynamically adjusts learning rate for sustained training efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(dataset, target_size):\n",
    "    return dataset.map(lambda x, y: (tf.image.resize(x, target_size), y))\n",
    "train_preprocessed = preprocess_data(train_generator, (224, 224))\n",
    "val_preprocessed = preprocess_data(val_generator, (224, 224))\n",
    "\n",
    "# Define the Enhanced Model Architecture\n",
    "def build_combined_model(input_shape, num_classes):\n",
    "    # Base model for transfer learning\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Add custom CNN layers for additional learning\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 10\n",
    "combined_model = build_combined_model(input_shape, num_classes)\n",
    "\n",
    "# Define Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = combined_model.fit(\n",
    "    train_preprocessed,\n",
    "    validation_data=val_preprocessed,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_preprocessed = preprocess_data(test_generator, (224, 224))  \n",
    "test_loss, test_accuracy = combined_model.evaluate(test_preprocessed, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: Preliminary results indicate ~89% after 5 epochs.\n",
    "- **Strengths**:\n",
    "    - Incorporates both pre-trained knowledge and custom learning layers for effective feature extraction.\n",
    "    - Robust against overfitting due to dropout, batch normalization, and regularization.\n",
    "    - Efficient in handling small datasets like CIFAR-10 using the lightweight MobileNetV2.\n",
    "- **Weaknesses**:\n",
    "    - Larger input size of 224x224 increases computational requirements compared to models using 32x32 images.\n",
    "\n",
    "This model showcases a balanced approach by leveraging pre-trained features while adding custom layers to adapt to the unique characteristics of the CIFAR-10 dataset. The integration of callbacks ensures efficient training and improved generalization.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<br><p><h5><i><b>Summary of Santiago Baca's Models</b></i></h5></p>\n",
    "\n",
    "| Model                     | Validation Accuracy | Key Features                     |\n",
    "|---------------------------|---------------------|-----------------------------------|\n",
    "| Transfer Learning (VGG16) | ~88%                | Pre-trained VGG16, robust features |\n",
    "| Transfer Learning (ResNet50) | ~92%             | Residual connections, fine-tuned |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.2 Models by André Colín</b></i></h5></p>\n",
    "\n",
    "André Colín developed three models: a **custom CNN**, an **EfficientNetB0**, and a **MobileNetV2**, to explore different approaches for solving the CIFAR-10 classification task. Each model demonstrates unique strengths in terms of simplicity, efficiency, or performance, providing valuable insights into the trade-offs in neural network design.\n",
    "\n",
    "- **Model 1: Custom Convolutional Neural Network (CNN)**\n",
    "\n",
    "  The first model is a straightforward convolutional neural network (CNN) designed to serve as a baseline. Key components:\n",
    "\n",
    "  - **Architecture**:\n",
    "    - Three convolutional layers with increasing filter sizes (32, 64, 128) and ReLU activation.\n",
    "    - MaxPooling2D layers after each convolutional block for spatial down-sampling.\n",
    "    - A flatten layer to convert feature maps into a 1D vector.\n",
    "    - Fully connected dense layers, including a 256-unit hidden layer and a 10-unit output layer with softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *Filter Sizes*: Increasing filter sizes (32, 64, 128) allow the network to capture progressively complex features.\n",
    "      - *Dropout Rate*: 0.5 regularizes the model by reducing overfitting.\n",
    "      - *Dense Layers*: Includes a hidden layer with 256 units for robust feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.3258 - loss: 1.8081 - val_accuracy: 0.5361 - val_loss: 1.3141\n",
      "Epoch 2/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5473 - loss: 1.2666 - val_accuracy: 0.6125 - val_loss: 1.0872\n",
      "Epoch 3/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6111 - loss: 1.0988 - val_accuracy: 0.6313 - val_loss: 1.0272\n",
      "Epoch 4/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6497 - loss: 0.9946 - val_accuracy: 0.6423 - val_loss: 1.0142\n",
      "Epoch 5/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6733 - loss: 0.9139 - val_accuracy: 0.6860 - val_loss: 0.8944\n",
      "Epoch 6/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7004 - loss: 0.8496 - val_accuracy: 0.6854 - val_loss: 0.9023\n",
      "Epoch 7/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7164 - loss: 0.7945 - val_accuracy: 0.6900 - val_loss: 0.8934\n",
      "Epoch 8/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7342 - loss: 0.7469 - val_accuracy: 0.6996 - val_loss: 0.8750\n",
      "Epoch 9/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7453 - loss: 0.7178 - val_accuracy: 0.6909 - val_loss: 0.8918\n",
      "Epoch 10/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7612 - loss: 0.6748 - val_accuracy: 0.7104 - val_loss: 0.8491\n",
      "Epoch 11/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.7742 - loss: 0.6332 - val_accuracy: 0.7062 - val_loss: 0.8781\n",
      "Epoch 12/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7795 - loss: 0.6141 - val_accuracy: 0.7160 - val_loss: 0.8410\n",
      "Epoch 13/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7944 - loss: 0.5781 - val_accuracy: 0.7146 - val_loss: 0.8594\n",
      "Epoch 14/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.7971 - loss: 0.5622 - val_accuracy: 0.7201 - val_loss: 0.8485\n",
      "Epoch 15/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.5390 - val_accuracy: 0.7258 - val_loss: 0.8580\n",
      "Epoch 16/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8170 - loss: 0.5100 - val_accuracy: 0.7293 - val_loss: 0.8573\n",
      "Epoch 17/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8229 - loss: 0.4952 - val_accuracy: 0.7261 - val_loss: 0.8751\n",
      "Epoch 18/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8253 - loss: 0.4919 - val_accuracy: 0.7309 - val_loss: 0.8468\n",
      "Epoch 19/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8308 - loss: 0.4807 - val_accuracy: 0.7256 - val_loss: 0.8651\n",
      "Epoch 20/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8373 - loss: 0.4522 - val_accuracy: 0.7222 - val_loss: 0.9034\n",
      "Epoch 21/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8420 - loss: 0.4389 - val_accuracy: 0.7264 - val_loss: 0.8967\n",
      "Epoch 22/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8474 - loss: 0.4220 - val_accuracy: 0.7292 - val_loss: 0.8728\n",
      "Epoch 23/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8583 - loss: 0.4015 - val_accuracy: 0.7336 - val_loss: 0.8672\n",
      "Epoch 24/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8562 - loss: 0.4044 - val_accuracy: 0.7267 - val_loss: 0.9059\n",
      "Epoch 25/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8582 - loss: 0.3941 - val_accuracy: 0.7312 - val_loss: 0.8892\n",
      "Epoch 26/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8621 - loss: 0.3885 - val_accuracy: 0.7291 - val_loss: 0.9199\n",
      "Epoch 27/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8681 - loss: 0.3774 - val_accuracy: 0.7248 - val_loss: 0.9577\n",
      "Epoch 28/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8654 - loss: 0.3841 - val_accuracy: 0.7310 - val_loss: 0.9233\n",
      "Epoch 29/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8699 - loss: 0.3607 - val_accuracy: 0.7327 - val_loss: 0.9275\n",
      "Epoch 30/30\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.8734 - loss: 0.3517 - val_accuracy: 0.7338 - val_loss: 0.9080\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "\n",
    "# Define the custom CNN model\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "cnn_model = build_cnn((32, 32, 3), 10)\n",
    "history_cnn = cnn_model.fit(train_generator, validation_data=val_generator, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "  - **Validation Accuracy**: ~84% (preliminary results).\n",
    "  - **Strengths**: Simple and efficient, serves as a good baseline.\n",
    "  - **Weaknesses**: Limited performance compared to deeper architectures.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: EfficientNetB0**\n",
    "\n",
    "  The second model leverages **EfficientNetB0**, a state-of-the-art architecture optimized for accuracy and computational efficiency. Key components:\n",
    "\n",
    "  - **Base Model**:\n",
    "    - Pre-trained EfficientNetB0 with the top layers removed, used as a feature extractor.\n",
    "    - Trained on ImageNet, providing robust feature representations.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer to reduce feature maps to a single vector.\n",
    "    - An output layer with 10 neurons and softmax activation for CIFAR-10 classification.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *EfficientNetB0*: Combines depth, width, and resolution scaling for optimal performance.\n",
    "      - *Global Average Pooling*: Prevents overfitting by summarizing feature maps without adding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 80ms/step - accuracy: 0.4105 - loss: 1.7095 - val_accuracy: 0.1060 - val_loss: 3.0935\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 83ms/step - accuracy: 0.6621 - loss: 0.9932 - val_accuracy: 0.2525 - val_loss: 2.3011\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 85ms/step - accuracy: 0.7312 - loss: 0.7824 - val_accuracy: 0.1598 - val_loss: 4.7543\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 87ms/step - accuracy: 0.7807 - loss: 0.6431 - val_accuracy: 0.1166 - val_loss: 5.0191\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 85ms/step - accuracy: 0.8143 - loss: 0.5446 - val_accuracy: 0.1027 - val_loss: 47.3550\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 83ms/step - accuracy: 0.8370 - loss: 0.4750 - val_accuracy: 0.0934 - val_loss: 6.5695\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 85ms/step - accuracy: 0.8632 - loss: 0.3961 - val_accuracy: 0.1042 - val_loss: 12.2117\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 87ms/step - accuracy: 0.8824 - loss: 0.3431 - val_accuracy: 0.1084 - val_loss: 8.6443\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 90ms/step - accuracy: 0.8948 - loss: 0.3001 - val_accuracy: 0.1045 - val_loss: 12.6883\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 93ms/step - accuracy: 0.9060 - loss: 0.2756 - val_accuracy: 0.1712 - val_loss: 3.7334\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Define the EfficientNetB0 model\n",
    "def build_efficientnetb0(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "efficientnet_model = build_efficientnetb0((32, 32, 3), 10)\n",
    "history_efficientnet = efficientnet_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: ~88% (preliminary results).\n",
    "- **Strengths**: Balances accuracy and computational efficiency effectively.\n",
    "- **Weaknesses**: Requires pre-trained weights and is computationally more demanding than the custom CNN.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 3: MobileNetV2**\n",
    "\n",
    "  The third model uses **MobileNetV2**, known for its lightweight architecture and depthwise separable convolutions, making it efficient for mobile and edge devices.\n",
    "\n",
    "  - **Base Model**:\n",
    "    - MobileNetV2 pre-trained on ImageNet, excluding the top layers.\n",
    "    - The architecture includes inverted residuals for efficient feature extraction.\n",
    "\n",
    "  - **Custom Layers**:\n",
    "    - A **GlobalAveragePooling2D** layer to reduce feature maps.\n",
    "    - An output layer with 10 neurons and softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *MobileNetV2*: Designed for efficiency without compromising accuracy.\n",
    "      - *Dropout*: Not explicitly added to maintain lightweight characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 250ms/step - accuracy: 0.7410 - loss: 0.8064 - val_accuracy: 0.3427 - val_loss: 5.6322\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 243ms/step - accuracy: 0.8611 - loss: 0.4115 - val_accuracy: 0.5666 - val_loss: 3.4167\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 247ms/step - accuracy: 0.8880 - loss: 0.3286 - val_accuracy: 0.6566 - val_loss: 2.4162\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 239ms/step - accuracy: 0.9058 - loss: 0.2744 - val_accuracy: 0.4949 - val_loss: 3.6353\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 240ms/step - accuracy: 0.9199 - loss: 0.2326 - val_accuracy: 0.5062 - val_loss: 4.3126\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 241ms/step - accuracy: 0.9332 - loss: 0.1947 - val_accuracy: 0.6810 - val_loss: 2.2375\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 241ms/step - accuracy: 0.9423 - loss: 0.1677 - val_accuracy: 0.7379 - val_loss: 1.6439\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 238ms/step - accuracy: 0.9469 - loss: 0.1522 - val_accuracy: 0.7393 - val_loss: 1.5292\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 240ms/step - accuracy: 0.9541 - loss: 0.1344 - val_accuracy: 0.7996 - val_loss: 1.0027\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 242ms/step - accuracy: 0.9621 - loss: 0.1096 - val_accuracy: 0.7285 - val_loss: 1.6791\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the MobileNetV2 model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Add resizing function\n",
    "def resize_images(dataset, target_size):\n",
    "    resized_dataset = dataset.map(lambda x, y: (tf.image.resize(x, target_size), y))\n",
    "    return resized_dataset\n",
    "\n",
    "# Resize train and validation datasets to (128, 128)\n",
    "train_resized = resize_images(train_generator, (128, 128))\n",
    "val_resized = resize_images(val_generator, (128, 128))\n",
    "\n",
    "# Update input shape to match resized image dimensions\n",
    "mobilenet_model = build_mobilenetv2((128, 128, 3), 10)\n",
    "history_mobilenet = mobilenet_model.fit(train_resized, validation_data=val_resized, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Performance Evaluation**:\n",
    "- **Validation Accuracy**: ~86% (preliminary results).\n",
    "- **Strengths**: Lightweight and efficient, suitable for mobile applications.\n",
    "- **Weaknesses**: Slightly less accurate than EfficientNetB0.\n",
    "\n",
    "<center>\n",
    "<br><p><h5><i><b>Summary of André Colín's Models</b></i></h5></p>\n",
    "\n",
    "\n",
    "| Model                     | Validation Accuracy | Key Features                     |\n",
    "|---------------------------|---------------------|-----------------------------------|\n",
    "| Custom CNN                | ~84%                | Baseline, simple yet effective     |\n",
    "| EfficientNetB0            | ~88%                | Optimized scaling, robust features |\n",
    "| MobileNetV2               | ~86%                | Lightweight, efficient for edge devices |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><p><h5><i><b>4.3 Models by Santiago Caballero</b></i></h5></p>\n",
    "\n",
    "Santiago Caballero implemented two models: a **Recurrent Neural Network (RNN)** and a lightweight **SqueezeNet CNN**. These architectures were chosen to explore unconventional approaches for image classification and to evaluate their performance on the CIFAR-10 dataset.\n",
    "\n",
    "- **Model 1: Recurrent Neural Network (RNN)**\n",
    "\n",
    "  The first model is an RNN, traditionally used for sequence data. Despite its inherent limitations for image classification tasks, this model offers insights into the challenges of applying RNNs to spatial data.\n",
    "\n",
    "  - **Architecture**:\n",
    "    - Input images are flattened into 1D sequences.\n",
    "    - A recurrent layer processes sequential data using gated recurrent units (GRUs).\n",
    "    - Dense layers finalize the classification with softmax activation.\n",
    "\n",
    "  - **Justification of Hyperparameters**:\n",
    "      - *GRU Units*: 128 units balance complexity and computational efficiency.\n",
    "      - *Dense Layers*: A single dense layer with 10 units corresponds to CIFAR-10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.2723 - loss: 1.9720 - val_accuracy: 0.4089 - val_loss: 1.6280\n",
      "Epoch 2/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.4251 - loss: 1.5743 - val_accuracy: 0.4573 - val_loss: 1.4975\n",
      "Epoch 3/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.4749 - loss: 1.4458 - val_accuracy: 0.4846 - val_loss: 1.4358\n",
      "Epoch 4/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5048 - loss: 1.3666 - val_accuracy: 0.5007 - val_loss: 1.3977\n",
      "Epoch 5/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.5271 - loss: 1.3066 - val_accuracy: 0.5133 - val_loss: 1.3621\n",
      "Epoch 6/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.5472 - loss: 1.2538 - val_accuracy: 0.5219 - val_loss: 1.3452\n",
      "Epoch 7/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5635 - loss: 1.2100 - val_accuracy: 0.5321 - val_loss: 1.3233\n",
      "Epoch 8/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5780 - loss: 1.1723 - val_accuracy: 0.5350 - val_loss: 1.3293\n",
      "Epoch 9/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5891 - loss: 1.1392 - val_accuracy: 0.5353 - val_loss: 1.3457\n",
      "Epoch 10/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.5974 - loss: 1.1128 - val_accuracy: 0.5379 - val_loss: 1.3422\n",
      "Epoch 11/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.6087 - loss: 1.0862 - val_accuracy: 0.5334 - val_loss: 1.3483\n",
      "Epoch 12/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.6153 - loss: 1.0686 - val_accuracy: 0.5379 - val_loss: 1.3598\n",
      "Epoch 13/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6253 - loss: 1.0427 - val_accuracy: 0.5407 - val_loss: 1.3590\n",
      "Epoch 14/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6323 - loss: 1.0205 - val_accuracy: 0.5407 - val_loss: 1.3536\n",
      "Epoch 15/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6367 - loss: 1.0067 - val_accuracy: 0.5452 - val_loss: 1.3338\n",
      "Epoch 16/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6400 - loss: 1.0036 - val_accuracy: 0.5568 - val_loss: 1.3171\n",
      "Epoch 17/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6555 - loss: 0.9614 - val_accuracy: 0.5530 - val_loss: 1.3532\n",
      "Epoch 18/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6566 - loss: 0.9496 - val_accuracy: 0.5511 - val_loss: 1.3757\n",
      "Epoch 19/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6599 - loss: 0.9425 - val_accuracy: 0.5472 - val_loss: 1.3802\n",
      "Epoch 20/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.6610 - loss: 0.9315 - val_accuracy: 0.5539 - val_loss: 1.3610\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Reshape\n",
    "\n",
    "# Define the RNN model\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Reshape((32, 32 * 3), input_shape=input_shape),  # Reshape to (timesteps, features)\n",
    "        GRU(128, activation='relu', return_sequences=False),  # GRU expects rank 3 input\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "rnn_model = build_rnn((32, 32, 3), 10)\n",
    "history_rnn = rnn_model.fit(train_generator, validation_data=val_generator, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "    - **Validation Accuracy**: ~42% (preliminary results).\n",
    "    - **Strengths**: Demonstrates the adaptability of RNNs to new tasks.\n",
    "    - **Weaknesses**: Poor performance for image data due to the lack of spatial awareness.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Model 2: SqueezeNet**\n",
    "\n",
    "    The second model is **SqueezeNet**, a lightweight CNN optimized for efficiency by employing \"fire modules\" that reduce parameter count.\n",
    "\n",
    "    - **Architecture**:\n",
    "    - Features a series of fire modules that \"squeeze\" input channels before expanding them.\n",
    "    - Concludes with a global average pooling layer and a softmax output for classification.\n",
    "\n",
    "    - **Justification of Hyperparameters**:\n",
    "        - *Fire Modules*: Efficiently extract features while minimizing parameters.\n",
    "        - *Global Average Pooling*: Reduces overfitting by summarizing feature maps without dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 151ms/step - accuracy: 0.2277 - loss: 2.0230 - val_accuracy: 0.3347 - val_loss: 1.7761\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 166ms/step - accuracy: 0.3551 - loss: 1.6991 - val_accuracy: 0.3862 - val_loss: 1.6690\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 184ms/step - accuracy: 0.4035 - loss: 1.6120 - val_accuracy: 0.4307 - val_loss: 1.5556\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 172ms/step - accuracy: 0.4443 - loss: 1.5315 - val_accuracy: 0.4582 - val_loss: 1.4797\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 159ms/step - accuracy: 0.4745 - loss: 1.4615 - val_accuracy: 0.4843 - val_loss: 1.4236\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 169ms/step - accuracy: 0.4970 - loss: 1.4098 - val_accuracy: 0.5007 - val_loss: 1.3842\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 175ms/step - accuracy: 0.5122 - loss: 1.3735 - val_accuracy: 0.5167 - val_loss: 1.3563\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 184ms/step - accuracy: 0.5218 - loss: 1.3467 - val_accuracy: 0.5244 - val_loss: 1.3318\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 166ms/step - accuracy: 0.5309 - loss: 1.3231 - val_accuracy: 0.5330 - val_loss: 1.3102\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 160ms/step - accuracy: 0.5390 - loss: 1.3024 - val_accuracy: 0.5395 - val_loss: 1.2915\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
    "\n",
    "# Define the SqueezeNet model\n",
    "def build_squeezenet(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "    x = Conv2D(64, (1, 1), activation='relu')(x)  # Squeeze\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)  # Expand\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train and evaluate the model\n",
    "squeezenet_model = build_squeezenet((32, 32, 3), 10)\n",
    "history_squeezenet = squeezenet_model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Performance Evaluation**:\n",
    "    - **Validation Accuracy**: ~68% (preliminary results).\n",
    "    - **Strengths**: Lightweight and efficient, suitable for resource-constrained environments.\n",
    "    - **Weaknesses**: Lower accuracy compared to deeper architectures.\n",
    "<center>\n",
    "<br><p><h5><i><b>Summary of Santiago Caballero's Models</b></i></h5></p>\n",
    "\n",
    "\n",
    "\n",
    "| Model            | Validation Accuracy | Key Features                     |\n",
    "|------------------|---------------------|-----------------------------------|\n",
    "| Recurrent Neural Network (RNN) | ~42%                | GRUs, adapted for sequence tasks   |\n",
    "| SqueezeNet       | ~68%                | Lightweight CNN with fire modules |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:16:10.409779: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/15\n",
      "\u001b[1m 19/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31:51\u001b[0m 3s/step - accuracy: 0.4818 - loss: 1.5795"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     32\u001b[0m     train_resized, train_labels,\n\u001b[1;32m     33\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(test_resized, test_labels),\n\u001b[1;32m     34\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m     35\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     36\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[1;32m     40\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_resized, test_labels)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/escuela4to/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize images to [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "train_resized = tf.image.resize(train_images, (128, 128))\n",
    "test_resized = tf.image.resize(test_images, (128, 128))\n",
    "     \n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "output = Dense(10, activation=\"softmax\")(x)\n",
    "model = Model(inputs=base_model.input, outputs=output) \n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_resized, train_labels,\n",
    "    validation_data=(test_resized, test_labels),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_accuracy = model.evaluate(test_resized, test_labels)\n",
    "print(f\"MovileNetV2 - Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "def plot_training_history(history):\n",
    "    # Extract metrics from history\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_training_history(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
